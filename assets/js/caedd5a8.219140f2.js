"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[529],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),c=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),p=c(n),d=o,h=p["".concat(l,".").concat(d)]||p[d]||m[d]||a;return n?r.createElement(h,s(s({ref:t},u),{},{components:n})):r.createElement(h,s({ref:t},u))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,s=new Array(a);s[0]=p;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var c=2;c<a;c++)s[c]=n[c];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},8754:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>c});var r=n(7462),o=(n(7294),n(3905));const a={sidebar_position:2},s="Methodology",i={unversionedId:"methodology",id:"methodology",title:"Methodology",description:"Assessing CI stability",source:"@site/docs/methodology.md",sourceDirName:".",slug:"/methodology",permalink:"/reassure/docs/methodology",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/methodology.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Installation and setup",permalink:"/reassure/docs/installation"},next:{title:"API",permalink:"/reassure/docs/api"}},l={},c=[{value:"Assessing CI stability",id:"assessing-ci-stability",level:2},{value:"Analyzing results",id:"analyzing-results",level:2}],u={toc:c};function m(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"methodology"},"Methodology"),(0,o.kt)("h2",{id:"assessing-ci-stability"},"Assessing CI stability"),(0,o.kt)("p",null,"During performance measurements we measure React component render times with microsecond precision using ",(0,o.kt)("inlineCode",{parentName:"p"},"React.Profiler"),". This means\nthat the same code will run faster or slower depending on the machine. For this reason,\nbaseline & current measurements need to be run on the same machine. Optimally, they should be run one after another."),(0,o.kt)("p",null,"Moreover, in order to achieve meaningful results your CI agent needs to have stable performance. It does not matter\nreally if your agent is fast or slow as long as it is consistent in its performance. That's why during the performance\ntests the agent should not be used for any other work that might impact measuring render times."),(0,o.kt)("p",null,"In order to help you assess your machine stability, you can use ",(0,o.kt)("inlineCode",{parentName:"p"},"reassure check-stability")," command. It runs performance\nmeasurements twice for the current code, so baseline and current measurements refer to the same code. In such case the\nexpected changes are 0% (no change). The degree of random performance changes will reflect the stability of your machine.\nThis command can be run both on CI and local machines."),(0,o.kt)("p",null,"Normally, the random changes should be below 5%. Results of 10% and more considered too high and mean that you should\nwork on tweaking your machine stability."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},(0,o.kt)("strong",{parentName:"p"},"Note"),": As a trick of last resort you can increase the ",(0,o.kt)("inlineCode",{parentName:"p"},"run")," option, from the default value of 10 to 20, 50 or even 100, for all or some of your tests, based on the assumption that more test runs will even out measurement fluctuations. That will however make your tests run even longer.")),(0,o.kt)("p",null,"You can refer to our example ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/callstack/reassure/blob/main/.github/workflows/stability.yml"},"GitHub workflow"),"."),(0,o.kt)("h2",{id:"analyzing-results"},"Analyzing results"),(0,o.kt)("p",{align:"center"},(0,o.kt)("img",{src:"https://github.com/callstack/reassure/raw/main/packages/reassure/docs/report-markdown.png",width:"920px",alt:"Markdown report"})),(0,o.kt)("p",null,"Looking at the example you can notice that test scenarios can be assigned to certain categories:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Significant Changes To Render Duration")," shows test scenario where the change is statistically significant and ",(0,o.kt)("strong",{parentName:"li"},"should")," be looked into as it marks a potential performance loss/improvement"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Meaningless Changes To Render Duration")," shows test scenarios where the change is not stastatistically significant"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Changes To Render Count")," shows test scenarios where render count did change"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Added Scenarios")," shows test scenarios which do not exist in the baseline measurements"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Removed Scenarios")," shows test scenarios which do not exist in the current measurements")))}m.isMDXComponent=!0}}]);