"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[102],{7885:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var t=s(4848),r=s(8453);const i={sidebar_position:2},o="Methodology",a={id:"methodology",title:"Methodology",description:"Assessing CI stability",source:"@site/docs/methodology.md",sourceDirName:".",slug:"/methodology",permalink:"/reassure/docs/methodology",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Installation and setup",permalink:"/reassure/docs/installation"},next:{title:"API",permalink:"/reassure/docs/api"}},l={},c=[{value:"Assessing CI stability",id:"assessing-ci-stability",level:2},{value:"Analyzing results",id:"analyzing-results",level:2},{value:"Results categorization",id:"results-categorization",level:3},{value:"Render issues (experimental)",id:"render-issues-experimental",level:3}];function h(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"methodology",children:"Methodology"}),"\n",(0,t.jsx)(n.h2,{id:"assessing-ci-stability",children:"Assessing CI stability"}),"\n",(0,t.jsxs)(n.p,{children:["During performance measurements we measure React component render times with microsecond precision using ",(0,t.jsx)(n.code,{children:"React.Profiler"}),". This means\nthat the same code will run faster or slower depending on the machine. For this reason,\nbaseline & current measurements need to be run on the same machine. Optimally, they should be run one after another."]}),"\n",(0,t.jsx)(n.p,{children:"Moreover, in order to achieve meaningful results your CI agent needs to have stable performance. It does not matter\nreally if your agent is fast or slow as long as it is consistent in its performance. That's why during the performance\ntests the agent should not be used for any other work that might impact measuring render times."}),"\n",(0,t.jsxs)(n.p,{children:["In order to help you assess your machine stability, you can use ",(0,t.jsx)(n.code,{children:"reassure check-stability"})," command. It runs performance\nmeasurements twice for the current code, so baseline and current measurements refer to the same code. In such case the\nexpected changes are 0% (no change). The degree of random performance changes will reflect the stability of your machine.\nThis command can be run both on CI and local machines."]}),"\n",(0,t.jsx)(n.p,{children:"Normally, the random changes should be below 5%. Results of 10% and more considered too high and mean that you should\nwork on tweaking your machine stability."}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": As a trick of last resort you can increase the ",(0,t.jsx)(n.code,{children:"run"})," option, from the default value of 10 to 20, 50 or even 100, for all or some of your tests, based on the assumption that more test runs will even out measurement fluctuations. That will however make your tests run even longer."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["You can refer to our example ",(0,t.jsx)(n.a,{href:"https://github.com/callstack/reassure/blob/main/.github/workflows/stability.yml",children:"GitHub workflow"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"analyzing-results",children:"Analyzing results"}),"\n",(0,t.jsx)("p",{align:"center",children:(0,t.jsx)("img",{src:"https://github.com/callstack/reassure/raw/main/packages/reassure/docs/report-markdown.png",width:"920px",alt:"Markdown report"})}),"\n",(0,t.jsx)(n.h3,{id:"results-categorization",children:"Results categorization"}),"\n",(0,t.jsx)(n.p,{children:"Looking at the example you can notice that test scenarios can be assigned to certain categories:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Significant Changes To Duration"})," shows test scenario where the performance change is statistically significant and ",(0,t.jsx)(n.strong,{children:"should"})," be looked into as it marks a potential performance loss/improvement"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Meaningless Changes To Duration"})," shows test scenarios where the performance change is not statistically significant"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Changes To Count"})," shows test scenarios where the render or execution count did change"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Added Scenarios"})," shows test scenarios which do not exist in the baseline measurements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Removed Scenarios"})," shows test scenarios which do not exist in the current measurements"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"render-issues-experimental",children:"Render issues (experimental)"}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"This feature is experimental, and its behavior might change without increasing the major version of the package."})}),"\n",(0,t.jsx)(n.p,{children:"Reassure analyses your components' render patterns during the initial test run (usually the warm-up run) to spot signs of potential issues."}),"\n",(0,t.jsx)(n.p,{children:"Currently, it's able to inform you about the following types of issues:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Initial updates"})," informs about the number of updates (= re-renders) that happened immediately (synchronously) after the mount (= initial render). This is most likely caused by ",(0,t.jsx)(n.code,{children:"useEffect"})," hook triggering immediate re-renders using set state. In the optimal case, the initial render should not cause immediate re-renders by itself. Next, renders should be caused by some external source: user action, system event, API call response, timers, etc."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Redundant updates"})," inform about renders that resulted in the same host element tree as the previous render. After each update, this check inspects the host element structure and compares it to the previous structure. If they are the same, the subsequent render could be avoided as it resulted in no visible change to the user."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"This feature is available only on React Native at this time"}),"\n",(0,t.jsx)(n.li,{children:"The host element tree comparison ignores references to event handlers. This means that differences in function props (e.g. event handlers) are ignored and only non-function props (e.g. strings, numbers, objects, arrays, etc.) are considered"}),"\n",(0,t.jsx)(n.li,{children:"The report includes the indices of redundant renders for easier diagnose, 0th render is the mount (initial render), renders 1 and later are updates (re-renders)"}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var t=s(6540);const r={},i=t.createContext(r);function o(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);